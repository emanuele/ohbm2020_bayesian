@book{fisher1935design,
    author = {Fisher, Ronald A.},
    citeulike-article-id = {7375015},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0028446909},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0028446909},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0028446909},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0028446909},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0028446909/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0028446909},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0028446909},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0028446909},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0028446909\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0028446909},
    edition = {1},
    howpublished = {Hardcover},
    isbn = {0028446909},
    location = {Edinburgh},
    posted-at = {2013-05-28 14:11:22},
    priority = {2},
    publisher = {Oliver and Boyd},
    title = {{The Design of Experiments}},
    url = {http://www.worldcat.org/isbn/0028446909},
    year = {1935}
}

@book{fisher1925statistical,
    author = {Fisher, R. A.},
    citeulike-article-id = {12375740},
    citeulike-linkout-0 = {http://books.google.it/books?id=4bTttAJR5kEC},
    posted-at = {2013-05-28 13:49:32},
    priority = {2},
    publisher = {Cosmo Publications},
    series = {Cosmo study guides},
    title = {{Statistical Methods for Research Workers}},
    url = {http://books.google.it/books?id=4bTttAJR5kEC},
    year = {1925}
}

@article{neyman1933problem,
    author = {Neyman, J. and Pearson, E. S.},
    citeulike-article-id = {12369064},
    citeulike-linkout-0 = {http://www.jstor.org/stable/91247},
    journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
    posted-at = {2013-05-27 10:13:30},
    priority = {2},
    publisher = {The Royal Society},
    title = {{On the Problem of the Most Efficient Tests of Statistical Hypotheses}},
    url = {http://www.jstor.org/stable/91247},
    volume = {231},
    year = {1933}
}

@article{neyman1928use2,
    author = {Neyman, J. and Pearson, E. S.},
    citeulike-article-id = {12369062},
    citeulike-linkout-0 = {http://www.jstor.org/stable/2332112},
    journal = {Biometrika},
    number = {3/4},
    posted-at = {2013-05-27 10:09:08},
    priority = {2},
    publisher = {Biometrika Trust},
    title = {{On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part II}},
    url = {http://www.jstor.org/stable/2332112},
    volume = {20A},
    year = {1928}
}

@article{neyman1928use1,
    author = {Neyman, J. and Pearson, E. S.},
    citeulike-article-id = {12369061},
    citeulike-linkout-0 = {http://www.jstor.org/stable/2331945},
    journal = {Biometrika},
    number = {1/2},
    posted-at = {2013-05-27 10:07:56},
    priority = {2},
    publisher = {Biometrika Trust},
    title = {{On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part I}},
    url = {http://www.jstor.org/stable/2331945},
    volume = {20A},
    year = {1928}
}

@article{goodman2008dirty,
    abstract = {{The P value is a measure of statistical evidence that appears in virtually all medical research papers. Its interpretation is made extraordinarily difficult because it is not part of any formal system of statistical inference. As a result, the P value's inferential meaning is widely and often wildly misconstrued, a fact that has been pointed out in innumerable papers and books appearing since at least the 1940s. This commentary reviews a dozen of these common misinterpretations and explains why each is wrong. It also reviews the possible consequences of these improper understandings or representations of its meaning. Finally, it contrasts the P value with its Bayesian counterpart, the Bayes' factor, which has virtually all of the desirable properties of an evidential measure that the P value lacks, most notably interpretability. The most serious consequence of this array of P-value misconceptions is the false belief that the probability of a conclusion being in error can be calculated from the data in a single experiment without reference to external evidence or the plausibility of the underlying mechanism.}},
    address = {Departments of Oncology, Epidemiology, and Biostatistics, Johns Hopkins Schools of Medicine and Public Health, Baltimore, MD.},
    author = {Goodman, Steven},
    citeulike-article-id = {3000903},
    citeulike-linkout-0 = {http://dx.doi.org/10.1053/j.seminhematol.2008.04.003},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/18582619},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=18582619},
    doi = {10.1053/j.seminhematol.2008.04.003},
    issn = {00371963},
    journal = {Seminars in Hematology},
    month = jul,
    number = {3},
    pages = {135--140},
    pmid = {18582619},
    posted-at = {2013-05-26 10:07:43},
    priority = {2},
    title = {{A Dirty Dozen: Twelve P-Value Misconceptions}},
    url = {http://dx.doi.org/10.1053/j.seminhematol.2008.04.003},
    volume = {45},
    year = {2008}
}

@article{haxby2012multivariate,
    abstract = {{
                In 2001, we published a paper on the representation of faces and objects in ventral temporal cortex that introduced a new method for fMRI analysis, which subsequently came to be called multivariate pattern analysis (MVPA). MVPA now refers to a diverse set of methods that analyze neural responses as patterns of activity that reflect the varying brain states that a cortical field or system can produce. This paper recounts the circumstances and events that led to the original study and later developments and innovations that have greatly expanded this approach to fMRI data analysis, leading to its widespread application.
                Copyright {\copyright} 2012 Elsevier Inc. All rights reserved.
            }},
    author = {Haxby, James V.},
    citeulike-article-id = {10841538},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.neuroimage.2012.03.016},
    citeulike-linkout-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3389290/},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/22425670},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=22425670},
    day = {15},
    doi = {10.1016/j.neuroimage.2012.03.016},
    issn = {1095-9572},
    journal = {NeuroImage},
    month = aug,
    number = {2},
    pages = {852--855},
    pmcid = {PMC3389290},
    pmid = {22425670},
    posted-at = {2013-05-26 09:32:50},
    priority = {2},
    title = {{Multivariate pattern analysis of fMRI: the early beginnings.}},
    url = {http://dx.doi.org/10.1016/j.neuroimage.2012.03.016},
    volume = {62},
    year = {2012}
}

@article{norman2006beyond,
    abstract = {{A key challenge for cognitive neuroscience is determining how mental representations map onto patterns of neural activity. Recently, researchers have started to address this question by applying sophisticated pattern-classification algorithms to distributed (multi-voxel) patterns of functional MRI data, with the goal of decoding the information that is represented in the subject's brain at a particular point in time. This multi-voxel pattern analysis (MVPA) approach has led to several impressive feats of mind reading. More importantly, MVPA methods constitute a useful new tool for advancing our understanding of neural information processing. We review how researchers are using MVPA methods to characterize neural coding and information processing in domains ranging from visual perception to memory search.}},
    address = {Department of Psychology, Princeton University, Green Hall, Washington Road, Princeton, NJ 08540, USA.},
    author = {Norman, Kenneth A. and Polyn, Sean M. and Detre, Greg J. and Haxby, James V.},
    citeulike-article-id = {848369},
    citeulike-linkout-0 = {http://www.cell.com/cognitive-sciences/abstract/S1364-6613(06)00184-7},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.tics.2006.07.005},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/16899397},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=16899397},
    citeulike-linkout-4 = {http://www.sciencedirect.com/science/article/B6VH9-4KKNNHN-8/2/81baf5c442ebeb9c01ba967c241035d8},
    day = {1},
    doi = {10.1016/j.tics.2006.07.005},
    issn = {1364-6613},
    journal = {Trends in cognitive sciences},
    month = sep,
    number = {9},
    pages = {424--430},
    pmid = {16899397},
    posted-at = {2013-05-26 09:31:03},
    priority = {2},
    publisher = {Elsevier Science,},
    title = {{Beyond mind-reading: multi-voxel pattern analysis of fMRI data.}},
    url = {http://dx.doi.org/10.1016/j.tics.2006.07.005},
    volume = {10},
    year = {2006}
}

@article{button2013power,
    abstract = {{A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.}},
    author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafo, Marcus R.},
    citeulike-article-id = {12256820},
    citeulike-linkout-0 = {http://dx.doi.org/10.1038/nrn3475},
    citeulike-linkout-1 = {http://dx.doi.org/10.1038/nrn3475},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/23571845},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=23571845},
    day = {10},
    doi = {10.1038/nrn3475},
    issn = {1471-003X},
    journal = {Nat Rev Neurosci},
    month = may,
    number = {5},
    pages = {365--376},
    pmid = {23571845},
    posted-at = {2013-04-29 11:45:25},
    priority = {2},
    publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
    title = {{Power failure: why small sample size undermines the reliability of neuroscience}},
    url = {http://dx.doi.org/10.1038/nrn3475},
    volume = {14},
    year = {2013}
}

@article{kononenko1991information,
    abstract = {{In the past few years many systems for learning decision rules from examples were developed. As different systems allow different types of answers when classifying new instances, it is difficult to appropriately evaluate the systems' classification power in comparison with other classification systems or in comparison with human experts. Classification accuracy is usually used as a measure of classification performance. This measure is, however, known to have several defects. A fair evaluation criterion should exclude the influence of the class probabilities which may enable a completely uninformed classifier to trivially achieve high classification accuracy. In this paper a method for evaluating the information score of a classifier's answers is proposed. It excludes the influence of prior probabilities, deals with various types of imperfect or probabilistic answers and can be used also for comparing the performance in different domains.}},
    author = {Kononenko, Igor and Bratko, Ivan},
    booktitle = {Machine Learning},
    citeulike-article-id = {12303790},
    citeulike-linkout-0 = {http://dx.doi.org/10.1023/a\%253a1022642017308},
    citeulike-linkout-1 = {http://link.springer.com/article/10.1023/A\%3A1022642017308},
    doi = {10.1023/a\%253a1022642017308},
    number = {1},
    pages = {67--80},
    posted-at = {2013-04-26 21:51:15},
    priority = {2},
    publisher = {Kluwer Academic Publishers-Plenum Publishers},
    title = {{Information-Based Evaluation Criterion for Classifier's Performance}},
    url = {http://dx.doi.org/10.1023/a\%253a1022642017308},
    volume = {6},
    year = {1991}
}

@article{maris2012statistical,
    abstract = {{
                This article describes the mechanics and rationale of four different approaches to the statistical testing of electrophysiological data: (1) the Neyman-Pearson approach, (2) the permutation-based approach, (3), the bootstrap-based approach, and (4) the Bayesian approach. These approaches are evaluated from the perspective of electrophysiological studies, which involve multivariate (i.e., spatiotemporal) observations in which source-level signals are picked up to a certain extent by all sensors. Besides formal statistical techniques, there are also techniques that do not involve probability calculations but are very useful in dealing with multivariate data (i.e., verification of data-based predictions, cross-validation, and localizers). Moreover, data-based decision making can also be informed by mechanistic evidence that is provided by the structure in the data.
                Copyright {\copyright} 2011 Society for Psychophysiological Research.
            }},
    author = {Maris, Eric},
    citeulike-article-id = {10157446},
    citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1469-8986.2011.01320.x},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/22176204},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=22176204},
    day = {1},
    doi = {10.1111/j.1469-8986.2011.01320.x},
    issn = {1540-5958},
    journal = {Psychophysiology},
    month = apr,
    number = {4},
    pages = {549--565},
    pmid = {22176204},
    posted-at = {2013-04-23 16:42:34},
    priority = {2},
    title = {{Statistical testing in electrophysiological studies.}},
    url = {http://dx.doi.org/10.1111/j.1469-8986.2011.01320.x},
    volume = {49},
    year = {2012}
}

@article{ojala2010permutation,
    abstract = {{We explore the framework of permutation-based p-values for assessing the performance of classifiers. In this paper we study two simple permutation tests. The first test assess whether the classifier has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classification problems in computational biology. The second test studies whether the classifier is exploiting the dependency between the features in classification; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classifier performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classifier performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classifier exploits the interdependency between the features in the data.}},
    author = {Ojala, Markus and Garriga, Gemma C.},
    citeulike-article-id = {12295140},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1859913},
    issn = {1532-4435},
    journal = {J. Mach. Learn. Res.},
    month = aug,
    pages = {1833--1863},
    posted-at = {2013-04-23 13:42:19},
    priority = {2},
    publisher = {JMLR.org},
    title = {{Permutation Tests for Studying Classifier Performance}},
    url = {http://portal.acm.org/citation.cfm?id=1859913},
    volume = {11},
    year = {2010}
}

@inproceedings{brodersen2010balanced,
    abstract = {{Evaluating the performance of a classification algorithm critically requires a measure of the degree to which unseen examples have been identified with their correct class labels. In practice, generalizability is frequently estimated by averaging the accuracies obtained on individual cross-validation folds. This procedure, however, is problematic in two ways. First, it does not allow for the derivation of meaningful confidence intervals. Second, it leads to an optimistic estimate when a biased classifier is tested on an imbalanced dataset. We show that both problems can be overcome by replacing the conventional point estimate of accuracy by an estimate of the posterior distribution of the balanced accuracy.}},
    author = {Brodersen, K. H. and Ong, Cheng S. and Stephan, K. E. and Buhmann, J. M.},
    booktitle = {Pattern Recognition (ICPR), 2010 20th International Conference on},
    citeulike-article-id = {12272581},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icpr.2010.764},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5597285},
    doi = {10.1109/icpr.2010.764},
    institution = {Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland},
    isbn = {978-1-4244-7542-1},
    issn = {1051-4651},
    month = aug,
    pages = {3121--3124},
    posted-at = {2013-04-15 14:45:57},
    priority = {2},
    publisher = {IEEE},
    title = {{The Balanced Accuracy and Its Posterior Distribution}},
    url = {http://dx.doi.org/10.1109/icpr.2010.764},
    year = {2010}
}

@article{gill1999insignificance,
    abstract = {{The current method of hypothesis testing in the social sciences is under intense criticism, yet most political scientists are unaware of the important issues being raised. Criticisms focus on the construction and interpretation of a procedure that has dominated the reporting of empirical results for over fifty years. There is evidence that null hypothesis significance testing as practiced in political science is deeply flawed and widely misunderstood. This is important since most empirical work argues the value of findings through the use of the null hypothesis significance test. In this article I review the history of the null hypothesis significance testing paradigm in the social sciences and discuss major problems, some of which are logical inconsistencies while others are more interpretive in nature. I suggest alter native techniques to convey effectively the importance of data-analytic findings. These recommendations are illustrated with examples using empirical political science publications.}},
    author = {Gill, Jeff},
    citeulike-article-id = {1530542},
    citeulike-linkout-0 = {http://dx.doi.org/10.1177/106591299905200309},
    citeulike-linkout-1 = {http://prq.sagepub.com/content/52/3/647.abstract},
    citeulike-linkout-2 = {http://prq.sagepub.com/content/52/3/647.full.pdf},
    citeulike-linkout-3 = {http://prq.sagepub.com/cgi/content/abstract/52/3/647},
    day = {01},
    doi = {10.1177/106591299905200309},
    issn = {1938-274X},
    journal = {Political Research Quarterly},
    month = sep,
    number = {3},
    pages = {647--674},
    posted-at = {2013-01-31 10:11:01},
    priority = {2},
    publisher = {SAGE Publications},
    title = {{The Insignificance of Null Hypothesis Significance Testing}},
    url = {http://dx.doi.org/10.1177/106591299905200309},
    volume = {52},
    year = {1999}
}

@article{berger2003could,
    abstract = {{Ronald Fisher advocated testing using p-values, Harold Jeffreys proposed use of objective posterior probabilities of hypotheses and Jerzy Neyman recommended testing with fixed error probabilities. Each was quite critical of the other approaches. Most troubling for statistics and science is that the three approaches can lead to quite different practical conclusions.

This article focuses on discussion of the conditional frequentist approach to testing, which is argued to provide the basis for a methodological unification of the approaches of Fisher, Jeffreys and Neyman. The idea is to follow Fisher in using p-values to define the "strength of evidence" in data and to follow his approach of conditioning on strength of evidence; then follow Neyman by computing Type I and Type II error probabilities, but do so conditional on the strength of evidence in the data. The resulting conditional frequentist error probabilities equal the objective posterior probabilities of the hypotheses advocated by Jeffreys.}},
    author = {Berger, James O.},
    citeulike-article-id = {1760546},
    citeulike-linkout-0 = {http://dx.doi.org/10.1214/ss/1056397485},
    citeulike-linkout-1 = {http://www.ams.org/mathscinet-getitem?mr=1997064},
    doi = {10.1214/ss/1056397485},
    issn = {0883-4237},
    journal = {Statistical Science},
    month = feb,
    mrnumber = {MR1997064},
    number = {1},
    pages = {1--32},
    posted-at = {2013-01-18 18:39:43},
    priority = {2},
    title = {{Could Fisher, Jeffreys and Neyman Have Agreed on Testing?}},
    url = {http://dx.doi.org/10.1214/ss/1056397485},
    volume = {18},
    year = {2003}
}

@article{christensen2005testing,
    abstract = {{This article presents a simple example that illustrates the key differences and similarities between the Fisherian, Neyman-Pearson, and Bayesian approaches to testing. Implications for more complex situations are also discussed.}},
    author = {Christensen, Ronald},
    citeulike-article-id = {155496},
    citeulike-linkout-0 = {http://dx.doi.org/10.1198/000313005x20871},
    citeulike-linkout-1 = {http://www.ingentaconnect.com/content/asa/tas/2005/00000059/00000002/art00001},
    citeulike-linkout-2 = {http://www.tandfonline.com/doi/abs/10.1198/000313005X20871},
    day = {1},
    doi = {10.1198/000313005x20871},
    issn = {0003-1305},
    journal = {The American Statistician},
    month = may,
    number = {2},
    pages = {121--126},
    posted-at = {2013-01-15 16:35:15},
    priority = {2},
    publisher = {Taylor \& Francis},
    title = {{Testing Fisher, Neyman, Pearson, and Bayes}},
    url = {http://dx.doi.org/10.1198/000313005x20871},
    volume = {59},
    year = {2005}
}

@inproceedings{olivetti2012testing,
    abstract = {{Machine learning is increasingly adopted in neuroimaging-based neuroscience studies. The paradigm of predicting the stimuli provided to the subject from the concurrent brain activity is known as "brain decoding" and accurate predictions support the hypothesis that the brain activity encodes those stimuli. When the stimulus categories are more than two it is not straightforward how to assess the amount of evidence in support of such an hypothesis. Moreover it is unclear how to distinguish between a classifier that discriminates each single class from the one that discriminates only among subsets of the classes. In this work we propose to recast the testing problem as a test of statistical independence between the predicted and the actual class labels. In this setting we propose a novel method to test whether the classifier is able to discriminate all classes or just subsets of them. We show experimental evidence of its efficacy both on simulated and on real data from an MEG experiment.}},
    author = {Olivetti, E. and Greiner, S. and Avesani, P.},
    booktitle = {Pattern Recognition in NeuroImaging (PRNI), 2012 International Workshop on},
    citeulike-article-id = {11689310},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/prni.2012.14},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6295927},
    doi = {10.1109/prni.2012.14},
    institution = {Neuroinf. Lab. (NILab), Bruno Kessler Found., Trento, Italy},
    isbn = {978-1-4673-2182-2},
    month = jul,
    pages = {57--60},
    posted-at = {2012-11-14 16:25:59},
    priority = {2},
    publisher = {IEEE},
    title = {{Testing Multiclass Pattern Discrimination}},
    url = {http://dx.doi.org/10.1109/prni.2012.14},
    year = {2012}
}

@article{fisher1955statistical,
    abstract = {{The attempt to reinterpret the common tests of significance used in scientific research as though they constituted some kind of acceptance procedure and led to "decisions" in Wald's sense, originated in several misapprehensions and has led, apparently, to several more. The three phrases examined here, with a view to elucidating the fallacies they embody, are: (i) "Repeated sampling from the same population", (ii) Errors of the "second kind", (iii) "Inductive behaviour". Mathematicians without personal contact with the Natural Sciences have often been misled by such phrases. The errors to which they lead are not always only numerical.}},
    author = {Fisher, Ronald},
    citeulike-article-id = {8650057},
    citeulike-linkout-0 = {http://dx.doi.org/10.2307/2983785},
    citeulike-linkout-1 = {http://www.jstor.org/stable/2983785},
    doi = {10.2307/2983785},
    issn = {00359246},
    journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
    number = {1},
    pages = {69--78},
    posted-at = {2012-06-25 09:48:21},
    priority = {2},
    publisher = {Blackwell Publishing for the Royal Statistical Society},
    title = {{Statistical Methods and Scientific Induction}},
    url = {http://dx.doi.org/10.2307/2983785},
    volume = {17},
    year = {1955}
}

@inbook{olivetti2012induction,
    abstract = {{Machine learning and pattern recognition techniques are increasingly
  adopted in neuroimaging-based neuroscience research. In many
  applications a classifier is trained on brain data in order to
  predict a variable of interest. Two leading examples are brain
  decoding and clinical diagnosis. Brain decoding consists of
  predicting stimuli or mental states from concurrent functional brain
  data. In clinical diagnosis it is the presence or absence of a given
  medical condition that is predicted from brain data. Observing
  accurate classification is considered to support the hypothesis of
  variable-related information within brain data.
  In this work we briefly review the literature on statistical tests
  for this kind of hypothesis testing problem. We claim that the
  current approaches to this hypothesis testing problem are suboptimal, 
  do not cover all useful settings, and that they could
  lead to wrong conclusions. We present a more accurate statistical
  test and provide examples of its superiority.}},
    author = {Olivetti, Emanuele and Greiner, Susanne and Avesani, Paolo},
    booktitle = {Machine Learning and Interpretation in Neuroimaging},
    citeulike-article-id = {10483017},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-642-34713-9\_6},
    doi = {10.1007/978-3-642-34713-9\_6},
    editor = {Langs, Georg and Rish, Irina and Grosse-Wentrup, Moritz and Murphy, Brian},
    isbn = {978-3-642-34712-2},
    pages = {42--50},
    posted-at = {2012-03-21 11:11:05},
    priority = {2},
    publisher = {Springer Berlin Heidelberg},
    series = {Lecture Notes in Compuer Science},
    title = {{Induction in Neuroscience with Classification: Issues and Solutions}},
    url = {http://dx.doi.org/10.1007/978-3-642-34713-9\_6},
    volume = {7263},
    year = {2012}
}

@article{gigerenzer2004mindless,
    abstract = {{Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the  ” null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.}},
    author = {Gigerenzer, Gerd},
    booktitle = {Statistical Significance},
    citeulike-article-id = {2572870},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.socec.2004.09.033},
    citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6W5H-4DS8FKY-4/2/155f1008d1dc5f0aed6e76ebaa2e8ba5},
    doi = {10.1016/j.socec.2004.09.033},
    issn = {10535357},
    journal = {The Journal of Socio-Economics},
    month = nov,
    number = {5},
    pages = {587--606},
    posted-at = {2011-10-12 09:18:39},
    priority = {2},
    title = {{Mindless statistics}},
    url = {http://dx.doi.org/10.1016/j.socec.2004.09.033},
    volume = {33},
    year = {2004}
}

@inbook{gigerenzer2004null,
    address = {Thousand Oaks (CA)},
    author = {Gigerenzer, Gerd and Krauss, Stefan and Vitouch, Oliver},
    booktitle = {The SAGE Handbook of Quantitative Methodology for the Social Sciences},
    citeulike-article-id = {9885607},
    citeulike-linkout-0 = {http://library.mpib-berlin.mpg.de/ft/gg/GG\_Null\_2004.pdf},
    editor = {Kaplan, David},
    isbn = {9781412986311},
    pages = {392--409},
    posted-at = {2011-10-10 09:05:57},
    priority = {2},
    publisher = {Sage},
    title = {{The null ritual : What you always wanted to know about significance testing but were afraid to ask}},
    url = {http://library.mpib-berlin.mpg.de/ft/gg/GG\_Null\_2004.pdf},
    year = {2004}
}

@article{haxby2001distributed,
    abstract = {{The functional architecture of the object vision pathway in the human brain was investigated using functional magnetic resonance imaging to measure patterns of response in ventral temporal cortex while subjects viewed faces, cats, five categories of man-made objects, and nonsense pictures. A distinct pattern of response was found for each stimulus category. The distinctiveness of the response to a given category was not due simply to the regions that responded maximally to that category, because the category being viewed also could be identified on the basis of the pattern of response when those regions were excluded from the analysis. Patterns of response that discriminated among all categories were found even within cortical regions that responded maximally to only one category. These results indicate that the representations of faces and objects in ventral temporal cortex are widely distributed and overlapping.}},
    author = {Haxby, James V. and Gobbini, M. Ida and Furey, Maura L. and Ishai, Alumit and Schouten, Jennifer L. and Pietrini, Pietro},
    citeulike-article-id = {1095075},
    citeulike-linkout-0 = {http://dx.doi.org/10.1126/science.1063736},
    citeulike-linkout-1 = {http://www.sciencemag.org/content/293/5539/2425.abstract},
    citeulike-linkout-2 = {http://www.sciencemag.org/content/293/5539/2425.full.pdf},
    citeulike-linkout-3 = {http://www.sciencemag.org/cgi/content/abstract/293/5539/2425},
    citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/11577229},
    citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=11577229},
    day = {28},
    doi = {10.1126/science.1063736},
    issn = {1095-9203},
    journal = {Science},
    month = sep,
    number = {5539},
    pages = {2425--2430},
    pmid = {11577229},
    posted-at = {2011-09-16 11:11:34},
    priority = {2},
    publisher = {American Association for the Advancement of Science},
    title = {{Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex}},
    url = {http://dx.doi.org/10.1126/science.1063736},
    volume = {293},
    year = {2001}
}

@article{berger1987testingprecise,
    abstract = {{Testing of precise (point or small interval) hypotheses is reviewed, with special emphasis placed on exploring the dramatic conflict between conditional measures (Bayes factors and posterior probabilities) and the classical P-value (or observed significance level). This conflict is highlighted by finding lower bounds on the conditional measures over wide classes of priors, in normal and binomial situations, lower bounds, which are much larger than the P-value; this leads to the recommendation of several alternatives to P-values. Results are also given concerning the validity of approximating an interval null by a point null. The overall discussion features critical examination of issues such as the probability of objective testing and the possibility of testing from confidence sets.}},
    author = {Berger, James O. and Delampady, Mohan},
    citeulike-article-id = {9699001},
    citeulike-linkout-0 = {http://dx.doi.org/10.2307/2245772},
    citeulike-linkout-1 = {http://www.jstor.org/stable/2245772},
    doi = {10.2307/2245772},
    issn = {08834237},
    journal = {Statistical Science},
    number = {3},
    posted-at = {2011-08-23 13:36:41},
    priority = {2},
    publisher = {Institute of Mathematical Statistics},
    title = {{Testing Precise Hypotheses}},
    url = {http://dx.doi.org/10.2307/2245772},
    volume = {2},
    year = {1987}
}

@article{bengio2004no,
    abstract = {{Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied.}},
    address = {Cambridge, MA, USA},
    author = {Bengio, Yoshua and Grandvalet, Yves},
    citeulike-article-id = {1206419},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1044695},
    issn = {1532-4435},
    journal = {J. Mach. Learn. Res.},
    month = dec,
    pages = {1089--1105},
    posted-at = {2011-08-05 11:43:11},
    priority = {2},
    publisher = {JMLR.org},
    title = {{No Unbiased Estimator of the Variance of K-Fold Cross-Validation}},
    url = {http://portal.acm.org/citation.cfm?id=1044695},
    volume = {5},
    year = {2004}
}

@inproceedings{olivetti2011testing,
    abstract = {{Is there "information" about the stimulus given to the
subject within brain data? The brain decoding approach tries to
answer this question by means of machine learning algorithms. A
classifier is learned from  a small sample of brain data
that is class-labeled according to the stimuli provided to the
subject during the experiment. The classifier is tested on
a different small sample, the test set, in order to observe the
number of misclassifications. The idea is that accurate
prediction provides evidence of the presence of information about
stimuli within brain data. In this work we show the connection
between information theory and learning theory in order to bridge
the gap between the initial information question and the observed
number of classification errors on a small test set. We propose a
hierarchical model about this connection and a related statistical
test about the presence of information. This test lies within the
Bayesian hypothesis testing framework and is compared against the
classical binomial test of the null hypothesis testing
framework. We show the empirical similarity between the two tests and
present an application on a real neuroimaging dataset about a covert
spatial attention task.}},
    author = {Olivetti, Emanuele and Veeramachaneni, Sriharsha and Avesani, Paolo},
    booktitle = {IEEE International Workshop on Pattern Recognition in NeuroImaging},
    citeulike-article-id = {9386522},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/PRNI.2011.14},
    doi = {10.1109/PRNI.2011.14},
    month = may,
    posted-at = {2011-06-06 10:05:50},
    priority = {2},
    title = {{Testing for Information with Brain Decoding}},
    url = {http://dx.doi.org/10.1109/PRNI.2011.14},
    year = {2011}
}

@article{hubbard2003confusion,
    abstract = {{Confusion surrounding the reporting and interpretation of results of classical statistical tests is widespread among applied researchers, most of whom erroneously believe that such tests are prescribed by a single coherent theory of statistical inference. This is not the case: Classical statistical testing is an anonymous hybrid of the competing and frequently contradictory approaches formulated by R. A. Fisher on the one hand, and Jerzy Neyman and Egon Pearson on the other. In particular, there is a widespread failure to appreciate the incompatibility of Fisher's evidential p value with the Type I error rate, α, of Neyman-Pearson statistical orthodoxy. The distinction between evidence (p's) and error (α's) is not trivial. Instead, it reflects the fundamental differences between Fisher's ideas on significance testing and inductive inference, and Neyman-Pearson's views on hypothesis testing and inductive behavior. The emphasis of the article is to expose this incompatibility, but we also briefly note a possible reconciliation.}},
    author = {Hubbard, Raymond and Bayarri, M. J.},
    citeulike-article-id = {5692598},
    citeulike-linkout-0 = {http://dx.doi.org/10.1198/0003130031856},
    citeulike-linkout-1 = {http://www.tandfonline.com/doi/abs/10.1198/0003130031856},
    day = {1},
    doi = {10.1198/0003130031856},
    issn = {0003-1305},
    journal = {The American Statistician},
    month = aug,
    number = {3},
    pages = {171--178},
    posted-at = {2011-02-26 16:31:24},
    priority = {2},
    publisher = {Taylor \& Francis},
    title = {{Confusion Over Measures of Evidence (p's) Versus Errors (alpha's) in Classical Statistical Testing}},
    url = {http://dx.doi.org/10.1198/0003130031856},
    volume = {57},
    year = {2003}
}

@article{berger1987testing,
    abstract = {{The problem of testing a point null hypothesis (or a "small interval" null hypothesis) is considered. Of interest is the relationship between the P value (or observed significance level) and conditional and Bayesian measures of evidence against the null hypothesis. Although one might presume that a small P value indicates the presence of strong evidence against the null, such is not necessarily the case. Expanding on earlier work [especially Edwards, Lindman, and Savage (1963) and Dickey (1977)], it is shown that actual evidence against a null (as measured, say, by posterior probability or comparative likelihood) can differ by an order of magnitude from the P value. For instance, data that yield a P value of .05, when testing a normal mean, result in a posterior probability of the null of at least .30 for any objective prior distribution. ("Objective" here means that equal prior weight is given the two hypotheses and that the prior is symmetric and nonincreasing away from the null; other definitions of "objective" will be seen to yield qualitatively similar results.) The overall conclusion is that P values can be highly misleading measures of the evidence provided by the data against the null hypothesis.}},
    author = {Berger, James O. and Sellke, Thomas},
    citeulike-article-id = {1294641},
    citeulike-linkout-0 = {http://dx.doi.org/10.2307/2289131},
    citeulike-linkout-1 = {http://www.jstor.org/stable/2289131},
    doi = {10.2307/2289131},
    issn = {01621459},
    journal = {Journal of the American Statistical Association},
    number = {397},
    pages = {112--122},
    posted-at = {2010-11-23 10:37:17},
    priority = {2},
    publisher = {American Statistical Association},
    title = {{Testing a Point Null Hypothesis: The Irreconcilability of P Values and Evidence}},
    url = {http://dx.doi.org/10.2307/2289131},
    volume = {82},
    year = {1987}
}

@article{sellke2001calibration,
    abstract = {{P values are the most commonly used tool to measure evidence against a hypothesis or hypothesized model. Unfortunately, they are often incorrectly viewed as an error probability for rejection of the hypothesis or, even worse, as the posterior probability that the hypothesis is true. The fact that these interpretations can be completely misleading when testing precise hypotheses is first reviewed, through consideration of two revealing simulations. Then two calibrations of a p value are developed, the first being interpretable as odds and the second as either a (conditional) frequentist error probability or as the posterior probability of the hypothesis.}},
    author = {Sellke, Thomas and Bayarri, M. J. and Berger, James O.},
    citeulike-article-id = {1760584},
    citeulike-linkout-0 = {http://dx.doi.org/10.2307/2685531},
    citeulike-linkout-1 = {http://www.jstor.org/stable/2685531},
    doi = {10.2307/2685531},
    issn = {00031305},
    journal = {The American Statistician},
    number = {1},
    pages = {62--71},
    posted-at = {2010-11-22 14:46:35},
    priority = {2},
    publisher = {American Statistical Association},
    title = {{Calibration of p Values for Testing Precise Null Hypotheses}},
    url = {http://dx.doi.org/10.2307/2685531},
    volume = {55},
    year = {2001}
}

@book{lehmann2005testing,
    abstract = {{This classic textbook, now available from Springer, summarizes developments in the field of hypotheses testing. Optimality considerations continue to provide the organizing principle. However, they are now tempered by a much stronger emphasis on the robustness properties of the resulting procedures. This book is an essential reference for any graduate student in statistics.}},
    author = {Lehmann, Erich L. and Romano, Joseph P.},
    citeulike-article-id = {1506124},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0387988645},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0387988645},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0387988645},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0387988645},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0387988645/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387988645},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0387988645},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0387988645},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0387988645\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0387988645},
    day = {04},
    edition = {3rd},
    howpublished = {Hardcover},
    isbn = {0387988645},
    month = apr,
    posted-at = {2010-11-16 16:06:25},
    priority = {2},
    publisher = {Springer},
    title = {{Testing Statistical Hypotheses (Springer Texts in Statistics)}},
    url = {http://www.worldcat.org/isbn/0387988645},
    year = {2005}
}

@inproceedings{olivetti2010brain,
    abstract = {{Classification-based approaches for data analysis are provoking wide interest and increasing adoption within the neuroscience community. Topics like "brain decoding", "multi-voxel patternanalysis" and "brain-computer interface" are prominent examples of this trend. The core problem of these investigations is hypothesis testing, i.e., finding evidence of some effect produced by the stimulation protocol within neural correlates. A classification algorithm is trained on the recorded data to learn how to discriminate between different stimuli. Then the misclassification rate of the predictions is estimated to answer the statistical test. This generic classification problem can be implemented in several ways depending on the exact neuroscientific question under investigation. However some implementations produce biased estimates due to circular analysis issues that could invalidate the conclusion of the scientific study. Therefore the most suited implementation of the classification problem must be used in order to avoid biases, to detect weak stimulus-related information within noise and to give the proper answer to the neuroscientific question at hand. In this work we propose different implementations of the classification-based approach in the case it comprises a variable selection step together with a classification step. For each different implementation we investigate the associated bias. Analyses are conducted on synthetic data and MEG data from a covert spatial attention task. The effects of different implementations of the classification algorithm are quantified by means of expected misclassification rate. Results prove the importance of adopting a proper error rate estimation process.}},
    author = {Olivetti, Emanuele and Mognon, Andrea and Greiner, Susanne and Avesani, Paolo},
    booktitle = {1st ICPR Workshop on Brain Decoding},
    citeulike-article-id = {7910968},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/WBD.2010.9},
    doi = {10.1109/WBD.2010.9},
    month = aug,
    posted-at = {2010-09-27 17:27:50},
    priority = {2},
    title = {{Brain Decoding: Biases in Error Estimation}},
    url = {http://dx.doi.org/10.1109/WBD.2010.9},
    year = {2010}
}

@book{jeffreys1961theory,
    abstract = {{Another title in the reissued Oxford Classic Texts in the Physical Sciences series, Jeffrey's Theory of Probability, first published in 1939, was the first to develop a fundamental theory of scientific inference based on the ideas of Bayesian statistics. His ideas were way ahead of their time and it is only in the past ten years that the subject of Bayes' factors has been significantly developed and extended. Until recently the two schools of statistics (Bayesian and Frequentist) were distinctly different and set apart. Recent work (aided by increased computer power and availability) has changed all that and today's graduate students and researchers all require an understanding of Bayesian ideas. This book is their starting point.}},
    author = {Jeffreys, Harold},
    citeulike-article-id = {2490275},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0198503687},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0198503687},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0198503687},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0198503687},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0198503687/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0198503687},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0198503687},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0198503687},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0198503687\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0198503687},
    day = {12},
    edition = {3},
    howpublished = {Paperback},
    isbn = {0198503687},
    month = nov,
    posted-at = {2010-04-27 08:56:48},
    priority = {2},
    publisher = {Oxford University Press, USA},
    title = {{Theory of Probability}},
    url = {http://www.worldcat.org/isbn/0198503687},
    year = {1961}
}

@article{haynes2006decoding,
    abstract = {{Recent advances in human neuroimaging have shown that it is possible to accurately decode a person's conscious experience based only on non-invasive measurements of their brain activity. Such 'brain reading' has mostly been studied in the domain of visual perception, where it helps reveal the way in which individual experiences are encoded in the human brain. The same approach can also be extended to other types of mental state, such as covert attitudes and lie detection. Such applications raise important ethical issues concerning the privacy of personal thought.}},
    author = {Haynes, John-Dylan and Rees, Geraint},
    citeulike-article-id = {707983},
    citeulike-linkout-0 = {http://dx.doi.org/10.1038/nrn1931},
    citeulike-linkout-1 = {http://dx.doi.org/10.1038/nrn1931},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/16791142},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=16791142},
    day = {01},
    doi = {10.1038/nrn1931},
    issn = {1471-003X},
    journal = {Nature Reviews Neuroscience},
    month = jul,
    number = {7},
    pages = {523--534},
    pmid = {16791142},
    posted-at = {2010-04-12 16:43:22},
    priority = {2},
    publisher = {Nature Publishing Group},
    title = {{Decoding mental states from brain activity in humans}},
    url = {http://dx.doi.org/10.1038/nrn1931},
    volume = {7},
    year = {2006}
}

@article{goodman1999toward2,
    abstract = {{
                Bayesian inference is usually presented as a method for determining how scientific belief should be modified by data. Although Bayesian methodology has been one of the most active areas of statistical development in the past 20 years, medical researchers have been reluctant to embrace what they perceive as a subjective approach to data analysis. It is little understood that Bayesian methods have a data-based core, which can be used as a calculus of evidence. This core is the Bayes factor, which in its simplest form is also called a likelihood ratio. The minimum Bayes factor is objective and can be used in lieu of the P value as a measure of the evidential strength. Unlike P values, Bayes factors have a sound theoretical foundation and an interpretation that allows their use in both inference and decision making. Bayes factors show that P values greatly overstate the evidence against the null hypothesis. Most important, Bayes factors require the addition of background knowledge to be transformed into inferences--probabilities that a given conclusion is right or wrong. They make the distinction clear between experimental evidence and inferential conclusions while providing a framework in which to combine prior with current evidence.
            }},
    address = {Johns Hopkins University School of Medicine, Baltimore, Maryland, USA. sgoodman@jhu.edu},
    author = {Goodman, S. N.},
    citeulike-article-id = {496972},
    citeulike-linkout-0 = {http://www.annals.org/content/130/12/1005.abstract},
    citeulike-linkout-1 = {http://www.annals.org/content/130/12/1005.full.pdf},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/10383350},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=10383350},
    day = {15},
    issn = {0003-4819},
    journal = {Annals of internal medicine},
    month = jun,
    number = {12},
    pages = {1005--1013},
    pmid = {10383350},
    posted-at = {2010-04-12 15:39:35},
    priority = {2},
    title = {{Toward evidence-based medical statistics. 2: The Bayes factor.}},
    url = {http://www.annals.org/content/130/12/1005.abstract},
    volume = {130},
    year = {1999}
}

@article{goodman1999toward1,
    abstract = {{An important problem exists in the interpretation of modern medical research data: Biological understanding and previous research play little formal role in the interpretation of quantitative results. This phenomenon is manifest in the discussion sections of research articles and ultimately can affect the reliability of conclusions. The standard statistical approach has created this situation by promoting the illusion that conclusions can be produced with certain "error rates," without consideration of information from outside the experiment. This statistical approach, the key components of which are P values and hypothesis tests, is widely perceived as a mathematically coherent approach to inference. There is little appreciation in the medical community that the methodology is an amalgam of incompatible elements, whose utility for scientific inference has been the subject of intense debate among statisticians for almost 70 years. This article introduces some of the key elements of that debate and traces the appeal and adverse impact of this methodology to the P value fallacy, the mistaken idea that a single number can capture both the long-run outcomes of an experiment and the evidential meaning of a single result. This argument is made as a prelude to the suggestion that another measure of evidence should be used--the Bayes factor, which properly separates issues of long-run behavior from evidential strength and allows the integration of background knowledge with statistical findings.}},
    author = {Goodman, S. N.},
    citeulike-article-id = {7008321},
    citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/0010383371},
    citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=0010383371},
    day = {15},
    issn = {0003-4819},
    journal = {Annals of internal medicine},
    month = jun,
    number = {12},
    pages = {995--1004},
    pmid = {0010383371},
    posted-at = {2010-04-12 15:39:04},
    priority = {2},
    title = {{Toward evidence-based medical statistics. 1: The P value fallacy.}},
    url = {http://view.ncbi.nlm.nih.gov/pubmed/0010383371},
    volume = {130},
    year = {1999}
}

@article{kass1995bayes,
    abstract = {{In a 1935 paper and in his book Theory of probability, Jeffresy developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpies was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: From Jeffrey's Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. Bayes factors are very general and do not require alternative models to be nested. Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. In "non-Bayesian significance tests. The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. Bayes factors are useful for guiding an evolutionary model-building process. It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.}},
    author = {Kass, Robert E. and Raftery, Adrian E.},
    citeulike-article-id = {1778922},
    citeulike-linkout-0 = {http://dx.doi.org/10.2307/2291091},
    citeulike-linkout-1 = {http://www.jstor.org/stable/2291091},
    doi = {10.2307/2291091},
    issn = {01621459},
    journal = {Journal of the American Statistical Association},
    month = jun,
    number = {430},
    pages = {773--795},
    posted-at = {2010-04-11 15:49:25},
    priority = {2},
    publisher = {American Statistical Association},
    title = {{Bayes Factors}},
    url = {http://dx.doi.org/10.2307/2291091},
    volume = {90},
    year = {1995}
}

@article{pereira2008machine,
    abstract = {{
                Interpreting brain image experiments requires analysis of complex, multivariate data. In recent years, one analysis approach that has grown in popularity is the use of machine learning algorithms to train classifiers to decode stimuli, mental states, behaviours and other variables of interest from fMRI data and thereby show the data contain information about them. In this tutorial overview we review some of the key choices faced in using this approach as well as how to derive statistically significant results, illustrating each point from a case study. Furthermore, we show how, in addition to answering the question of 'is there information about a variable of interest' (pattern discrimination), classifiers can be used to tackle other classes of question, namely 'where is the information' (pattern localization) and 'how is that information encoded' (pattern characterization).
            }},
    author = {Pereira, F. and Mitchell, T. and Botvinick, M.},
    citeulike-article-id = {3809491},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.neuroimage.2008.11.007},
    citeulike-linkout-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892746/},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/19070668},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=19070668},
    day = {21},
    doi = {10.1016/j.neuroimage.2008.11.007},
    issn = {1095-9572},
    journal = {NeuroImage},
    month = mar,
    number = {1},
    pages = {199--209},
    pmcid = {PMC2892746},
    pmid = {19070668},
    posted-at = {2010-03-30 22:49:16},
    priority = {2},
    title = {{Machine learning classifiers and fMRI: a tutorial overview.}},
    url = {http://dx.doi.org/10.1016/j.neuroimage.2008.11.007},
    volume = {45},
    year = {2009}
}

@article{vangerven2009attention,
    abstract = {{Research on brain-computer interfaces (BCIs) is gaining strong interest. This is motivated by BCIs being applicable for helping disabled, for gaming, and as a tool in cognitive neuroscience. Often, motor imagery is used to produce (binary) control signals. However, finding other types of control signals that allow the discrimination of multiple classes would help to increase the applicability of BCIs. We have investigated if modulation of posterior alpha activity by means of covert spatial attention in two dimensions can be reliably classified in single trials. Magnetoencephalography (MEG) data were collected for 15 subjects who were engaged in a task where they covertly had to visually attend left, right, up or down during a period of 2500 ms. We then classified the trials using support vector machines. The four orientations of covert attention could be reliably classified up to a maximum of 69\% correctly classified trials (25\% chance level) without the need for lengthy and burdensome subject training. Low classification performance in some subjects was explained by a low alpha signal. These findings support the case that modulation of alpha activity by means of covert spatial attention is promising as a control signal for a two-dimensional BCI.}},
    author = {van Gerven, Marcel and Jensen, Ole},
    citeulike-article-id = {6425705},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.jneumeth.2009.01.016},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/19428515},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=19428515},
    day = {30},
    doi = {10.1016/j.jneumeth.2009.01.016},
    issn = {1872-678X},
    journal = {Journal of neuroscience methods},
    month = apr,
    number = {1},
    pages = {78--84},
    pmid = {19428515},
    posted-at = {2010-01-12 17:20:44},
    priority = {2},
    title = {{Attention modulations of posterior alpha as a control signal for two-dimensional brain-computer interfaces}},
    url = {http://dx.doi.org/10.1016/j.jneumeth.2009.01.016},
    volume = {179},
    year = {2009}
}

@Article{olivetti2020multiclass,
  author = 	 {Emanuele Olivetti},
  title = 	 {Multiclass Decoding and Bayesian Hypothesis Testing},
  journal = 	 {in preparation},
  year = 	 {2020},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

